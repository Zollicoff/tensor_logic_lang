# Tensor Logic Language - Architecture

> Based on Pedro Domingos' paper "Tensor Logic: The Language of AI"

## Core Philosophy

> "The sole construct in tensor logic is the tensor equation"

This is a **standalone compiled language** - not a library, not an interpreter. It produces native executables via LLVM.

```
program.tl  â†’  tlc compile  â†’  LLVM IR  â†’  clang  â†’  native binary
```

## Two Inference Modes (Both Compiled)

The paper describes two inference strategies. Both compile to native code:

### Forward Chaining (Implemented)
- Compiles to **nested loops**
- Computes ALL tensor elements eagerly
- Iterates until fixpoint for recursive equations
- Optimal for: neural networks, dense tensors

### Backward Chaining (Planned)
- Compiles to **recursive functions**
- Computes ONLY what's queried (demand-driven)
- Memoization to avoid redundant computation
- Optimal for: logic queries, sparse knowledge bases

```
# Forward: compute everything
Ancestor?           # â†’ loops over all i,j

# Backward: query-driven
Ancestor[0,5]?      # â†’ recursive call, traces dependencies
```

## Pipeline

```
.tl source
    â†“
[Frontend: Lexer â†’ Parser â†’ AST â†’ Type Check]
    â†“
[Codegen: AST â†’ LLVM IR]
    â”‚
    â”œâ”€â”€ Forward mode â†’ nested loops + fixpoint
    â””â”€â”€ Backward mode â†’ recursive functions + memoization
    â†“
[LLVM: clang]
    â†“
Native executable
```

## Source Structure

```
src/
â”œâ”€â”€ main.zig              # CLI entry point
â”‚
â”œâ”€â”€ frontend/             # Parsing and analysis
â”‚   â”œâ”€â”€ lexer.zig         # Tokenization
â”‚   â”œâ”€â”€ parser.zig        # Recursive descent parser
â”‚   â”œâ”€â”€ ast.zig           # AST node definitions
â”‚   â”œâ”€â”€ tokens.zig        # Token types
â”‚   â”œâ”€â”€ types.zig         # Type checking and inference
â”‚   â””â”€â”€ optimize.zig      # AST optimization passes
â”‚
â”œâ”€â”€ codegen/              # LLVM IR generation
â”‚   â”œâ”€â”€ llvm.zig          # Main orchestrator
â”‚   â”œâ”€â”€ autodiff.zig      # Computation graph, gradient derivation
â”‚   â”œâ”€â”€ einsum.zig        # Einstein summation (forward: loops)
â”‚   â”œâ”€â”€ backward.zig      # Backward chaining (recursive fns + memoization)
â”‚   â”œâ”€â”€ softmax.zig       # Softmax with reduction
â”‚   â”œâ”€â”€ layernorm.zig     # Layer normalization
â”‚   â”œâ”€â”€ concat.zig        # Concatenation for attention heads
â”‚   â”œâ”€â”€ fixpoint.zig      # Recursive equation convergence
â”‚   â”œâ”€â”€ sparse.zig        # Sparse tensor support
â”‚   â”œâ”€â”€ tucker.zig        # Tucker decomposition for sparseâ†’dense scaling
â”‚   â”œâ”€â”€ bp.zig            # Belief propagation helpers
â”‚   â”œâ”€â”€ gpu.zig           # GPU backends (CUDA/Metal templates)
â”‚   â”œâ”€â”€ tensor.zig        # Tensor allocation and indexing
â”‚   â”œâ”€â”€ expr.zig          # Expression evaluation
â”‚   â””â”€â”€ types.zig         # Shared types
â”‚
â””â”€â”€ lsp/                  # IDE support
    â””â”€â”€ server.zig        # VS Code language server
```

## Implementation Status

### Complete âœ…
- Lexer, parser, AST, type checker
- Einstein summation with implicit contraction
- All nonlinearities: step, relu, sigmoid, tanh, softmax, lnorm, exp, log, sqrt, abs, sin, cos
- Accumulation operators: `=`, `+=`, `max=`, `min=`, `*=`, `avg=`
- Division indices `X/2` for pooling
- Slice indices `X[4:8]` for subranges
- Concat for attention head merging
- Forward chaining with fixpoint iteration
- **Backward chaining with memoization** (recursive functions for query-driven inference)
- **Full autodiff**: tanh, exp, log, softmax gradients
- **Temperature sigmoid**: `sigmoid(x, T)` for embedding space reasoning
- **Sparse tensor support** (COO format allocation)
- **File I/O**: `save`/`load` for tensor persistence
- Virtual indices `*t`, primed indices `p'`, index arithmetic `i+1`
- VS Code extension with LSP
- **Tucker decomposition**: `tucker T(r1, r2, r3) from Source` for sparseâ†’dense scaling
- **Belief propagation**: Loopy BP is forward chaining (fixpoint + bp.zig helpers)

### In Progress ðŸ”§
- GPU backends (CUDA/Metal) - kernel templates created, full runtime pending

## Paper Features Mapping

| Paper Feature | Status | Implementation |
|--------------|--------|----------------|
| Tensor equations | âœ… | Core syntax |
| Einstein summation | âœ… | einsum.zig |
| Forward chaining | âœ… | loops + fixpoint |
| Backward chaining | âœ… | recursive functions + memoization |
| Autodiff | âœ… | autodiff.zig (full) |
| Sparse tensors | âœ… | sparse.zig (COO format) |
| Temperature Ïƒ(x,T) | âœ… | sigmoid(x, T) for embedding reasoning |
| Tucker decomposition | âœ… | tucker.zig (core tensor + factor matrices) |
| Belief propagation | âœ… | fixpoint.zig + bp.zig (loopy BP = forward chaining) |
| GPU acceleration | ðŸ”§ | gpu.zig (CUDA/Metal templates) |

## CLI

```bash
tlc build program.tl -o program   # Compile to native binary
tlc compile program.tl -o out.ll  # Compile to LLVM IR
tlc check program.tl              # Type check only
tlc lex program.tl                # Show tokens
tlc parse program.tl              # Show AST
```

## Design Principles

1. **One construct**: Everything is a tensor equation
2. **Compiled**: Native binaries via LLVM, no interpreter
3. **Two modes**: Forward (loops) and backward (recursion) chaining
4. **Declarative**: Equations state what, not how
5. **Differentiable**: Gradients are also tensor equations
6. **Faithful to paper**: 100% implementation of Domingos' Tensor Logic
