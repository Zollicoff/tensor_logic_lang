// LLVM IR Code Generator for Tensor Logic
//
// Main orchestrator for LLVM code generation. Imports specialized modules:
// - types.zig:    Shared data structures (TensorInfo, IndexVar)
// - tensor.zig:   Tensor allocation and indexing
// - expr.zig:     Expression evaluation
// - einsum.zig:   Einstein summation loop generation
// - softmax.zig:  Softmax with reduction
// - fixpoint.zig: Recursive equation handling
//
// Compile output with: clang output.ll -o program -lm

const std = @import("std");
const ast = @import("../frontend/ast.zig");

// Import codegen modules
const types = @import("types.zig");
const tensor_mod = @import("tensor.zig");
const expr_mod = @import("expr.zig");
const einsum = @import("einsum.zig");
const softmax = @import("softmax.zig");
const layernorm = @import("layernorm.zig");
const fixpoint = @import("fixpoint.zig");
const autodiff = @import("autodiff.zig");

// Re-export types
pub const TensorInfo = types.TensorInfo;
pub const IndexVar = types.IndexVar;

/// LLVM IR emitter - the main codegen context
pub const LLVMCodegen = struct {
    allocator: std.mem.Allocator,
    output: std.ArrayListUnmanaged(u8),

    // Arena for temporary strings (temps, labels, etc.)
    string_arena: std.heap.ArenaAllocator,

    // Counters for unique names
    temp_counter: usize,
    label_counter: usize,
    equation_counter: usize,

    // Track tensors and domains
    tensors: std.StringHashMapUnmanaged(TensorInfo),
    domains: std.StringHashMapUnmanaged(usize),

    // Pre-computed max dimensions for tensors (from scanning all constant indices)
    tensor_max_dims: std.StringHashMapUnmanaged([]usize),

    pub fn init(allocator: std.mem.Allocator) LLVMCodegen {
        return .{
            .allocator = allocator,
            .output = .{},
            .string_arena = std.heap.ArenaAllocator.init(allocator),
            .temp_counter = 0,
            .label_counter = 0,
            .equation_counter = 0,
            .tensors = .{},
            .domains = .{},
            .tensor_max_dims = .{},
        };
    }

    pub fn deinit(self: *LLVMCodegen) void {
        self.output.deinit(self.allocator);
        self.string_arena.deinit();
        var iter = self.tensors.iterator();
        while (iter.next()) |entry| {
            self.allocator.free(entry.value_ptr.dims);
            self.allocator.free(entry.value_ptr.strides);
        }
        self.tensors.deinit(self.allocator);
        self.domains.deinit(self.allocator);
        var max_iter = self.tensor_max_dims.iterator();
        while (max_iter.next()) |entry| {
            self.allocator.free(entry.value_ptr.*);
        }
        self.tensor_max_dims.deinit(self.allocator);
    }

    // =========================================================================
    // Core emit helpers
    // =========================================================================

    pub fn newTemp(self: *LLVMCodegen) ![]const u8 {
        const arena = self.string_arena.allocator();
        const name = try std.fmt.allocPrint(arena, "%t{d}", .{self.temp_counter});
        self.temp_counter += 1;
        return name;
    }

    pub fn newLabel(self: *LLVMCodegen) ![]const u8 {
        const arena = self.string_arena.allocator();
        const name = try std.fmt.allocPrint(arena, "L{d}", .{self.label_counter});
        self.label_counter += 1;
        return name;
    }

    pub fn emit(self: *LLVMCodegen, s: []const u8) !void {
        try self.output.appendSlice(self.allocator, s);
    }

    pub fn emitFmt(self: *LLVMCodegen, comptime fmt: []const u8, args: anytype) !void {
        try self.output.writer(self.allocator).print(fmt, args);
    }

    // =========================================================================
    // Main generation entry point
    // =========================================================================

    /// Generate complete LLVM IR module
    pub fn generate(self: *LLVMCodegen, program: *const ast.Program) ![]const u8 {
        // First pass: collect domains
        for (program.statements) |stmt| {
            if (stmt == .domain_decl) {
                const d = stmt.domain_decl;
                if (d.size) |size| {
                    try self.domains.put(self.allocator, d.name, @intCast(size));
                }
            }
        }

        // Second pass: compute max dimensions for each tensor from constant indices
        for (program.statements) |stmt| {
            if (stmt == .equation) {
                const eq = stmt.equation;
                try tensor_mod.updateTensorMaxDims(self, eq.lhs.name, eq.lhs.indices);
            }
        }

        // Module header
        try self.emit(
            \\; Tensor Logic Compiled Program
            \\; Generated by tlc (Tensor Logic Compiler)
            \\
            \\target triple = "x86_64-unknown-linux-gnu"
            \\
            \\; External declarations
            \\declare ptr @malloc(i64)
            \\declare ptr @calloc(i64, i64)
            \\declare void @free(ptr)
            \\declare i32 @printf(ptr, ...)
            \\declare double @llvm.fabs.f64(double)
            \\declare double @llvm.sqrt.f64(double)
            \\declare double @llvm.exp.f64(double)
            \\declare double @llvm.log.f64(double)
            \\declare double @llvm.sin.f64(double)
            \\declare double @llvm.cos.f64(double)
            \\declare double @llvm.pow.f64(double, double)
            \\declare double @llvm.fma.f64(double, double, double)
            \\
            \\; Format strings
            \\@.str.tensor_start = private constant [12 x i8] c"%s[%zu] = [\00"
            \\@.str.val = private constant [6 x i8] c"%.4g \00"
            \\@.str.end = private constant [3 x i8] c"]\0A\00"
            \\@.str.newline = private constant [2 x i8] c"\0A\00"
            \\
        );

        // Generate tensor name strings (once per unique tensor)
        var emitted_names = std.StringHashMapUnmanaged(void){};
        defer emitted_names.deinit(self.allocator);

        for (program.statements) |stmt| {
            switch (stmt) {
                .equation => |eq| {
                    if (!emitted_names.contains(eq.lhs.name)) {
                        try emitted_names.put(self.allocator, eq.lhs.name, {});
                        try self.emitFmt("@.name.{s} = private constant [{d} x i8] c\"{s}\\00\"\n", .{
                            eq.lhs.name,
                            eq.lhs.name.len + 1,
                            eq.lhs.name,
                        });
                    }
                },
                .backward_stmt => |b| {
                    // Pre-declare gradient tensor names
                    for (b.params) |param| {
                        const grad_name = try std.fmt.allocPrint(self.string_arena.allocator(), "dL_d{s}", .{param});
                        if (!emitted_names.contains(grad_name)) {
                            try emitted_names.put(self.allocator, grad_name, {});
                            try self.emitFmt("@.name.{s} = private constant [{d} x i8] c\"{s}\\00\"\n", .{
                                grad_name,
                                grad_name.len + 1,
                                grad_name,
                            });
                        }
                    }
                },
                else => {},
            }
        }

        try self.emit("\n");

        // Main function
        try self.emit(
            \\define i32 @main() {
            \\entry:
            \\
        );

        // Analyze statements: separate recursive from non-recursive
        var recursive_eqs = std.ArrayListUnmanaged(usize){};
        defer recursive_eqs.deinit(self.allocator);

        for (program.statements, 0..) |stmt, idx| {
            if (stmt == .equation) {
                const eq = stmt.equation;
                if (fixpoint.isRecursive(&eq)) {
                    try recursive_eqs.append(self.allocator, idx);
                }
            }
        }

        // Generate non-recursive statements first (excluding queries)
        for (program.statements, 0..) |stmt, idx| {
            // Skip queries (they come last) and recursive equations
            if (stmt == .query) continue;

            var is_recursive = false;
            for (recursive_eqs.items) |rec_idx| {
                if (rec_idx == idx) {
                    is_recursive = true;
                    break;
                }
            }
            if (!is_recursive) {
                try self.genStatement(&stmt, program);
            }
        }

        // Generate fixpoint loop for recursive equations
        if (recursive_eqs.items.len > 0) {
            try fixpoint.genFixpointLoop(self, program, recursive_eqs.items);
        }

        // Generate queries last (after fixpoint converges)
        for (program.statements) |stmt| {
            if (stmt == .query) {
                try self.genStatement(&stmt, program);
            }
        }

        // Free tensors
        var iter = self.tensors.iterator();
        while (iter.next()) |entry| {
            try self.emitFmt("    call void @free(ptr {s})\n", .{entry.value_ptr.llvm_ptr});
        }

        try self.emit(
            \\    ret i32 0
            \\}
            \\
        );

        return self.output.items;
    }

    // =========================================================================
    // Statement generation
    // =========================================================================

    fn genStatement(self: *LLVMCodegen, stmt: *const ast.Statement, program: *const ast.Program) !void {
        switch (stmt.*) {
            .domain_decl => {}, // Already processed
            .equation => |eq| try self.genEquation(&eq),
            .query => |q| try self.genQuery(&q),
            .sparse_decl => |s| try self.genSparseDecl(&s),
            .backward_stmt => |b| try self.genBackward(&b, program),
            else => {},
        }
    }

    /// Generate code for a tensor equation (the core of tensor logic)
    fn genEquation(self: *LLVMCodegen, eq: *const ast.Equation) !void {
        try self.emitFmt("\n    ; Equation: {s}[...] = ...\n", .{eq.lhs.name});

        // Check if this is a softmax or layer norm operation
        if (eq.rhs.* == .nonlinearity) {
            const nl = eq.rhs.nonlinearity;
            if (nl.func == .softmax) {
                try softmax.genSoftmax(self, eq);
                return;
            }
            if (nl.func == .lnorm) {
                try layernorm.genLayerNorm(self, eq);
                return;
            }
        }

        // Ensure LHS tensor exists
        if (!self.tensors.contains(eq.lhs.name)) {
            try tensor_mod.allocateTensor(self, eq.lhs.name, eq.lhs.indices);
        }

        // Analyze the equation to find all indices
        var all_indices = std.StringHashMapUnmanaged(usize){};
        defer all_indices.deinit(self.allocator);

        // Collect LHS indices (free indices), including normalize indices
        var lhs_indices = std.StringHashMapUnmanaged(void){};
        defer lhs_indices.deinit(self.allocator);
        for (eq.lhs.indices) |idx| {
            const name = switch (idx) {
                .name => |n| n,
                .normalize => |n| n,
                else => continue,
            };
            try lhs_indices.put(self.allocator, name, {});
            const size = self.domains.get(name) orelse 10;
            try all_indices.put(self.allocator, name, size);
        }

        // Collect RHS indices
        try expr_mod.collectExprIndices(self, eq.rhs, &all_indices);

        // Check if this is a simple scalar/broadcast assignment
        if (all_indices.count() == 0) {
            try einsum.genScalarAssign(self, eq);
            return;
        }

        // Build index variable list, marking contracted indices
        var index_vars = std.ArrayListUnmanaged(IndexVar){};
        defer index_vars.deinit(self.allocator);

        // Free indices first (appear on LHS)
        for (eq.lhs.indices) |idx| {
            if (idx == .name) {
                const size = all_indices.get(idx.name) orelse 10;
                try index_vars.append(self.allocator, .{
                    .name = idx.name,
                    .size = size,
                    .llvm_var = "",
                    .is_contracted = false,
                });
            }
        }

        // Contracted indices (on RHS but not LHS)
        var iter = all_indices.iterator();
        while (iter.next()) |entry| {
            if (!lhs_indices.contains(entry.key_ptr.*)) {
                try index_vars.append(self.allocator, .{
                    .name = entry.key_ptr.*,
                    .size = entry.value_ptr.*,
                    .llvm_var = "",
                    .is_contracted = true,
                });
            }
        }

        // Generate nested loops
        try einsum.genEinsumLoops(self, eq, index_vars.items);
    }

    fn genQuery(self: *LLVMCodegen, query: *const ast.Query) !void {
        const info = self.tensors.get(query.tensor.name) orelse return;

        try self.emitFmt("\n    ; Query: {s}\n", .{query.tensor.name});

        // Print tensor name
        try self.emitFmt("    call i32 (ptr, ...) @printf(ptr @.str.tensor_start, ptr @.name.{s}, i64 {d})\n", .{ query.tensor.name, info.totalSize() });

        // Print values (limit to first 20 for readability)
        const num_print = @min(info.totalSize(), 20);
        for (0..num_print) |i| {
            const ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ ptr, info.llvm_ptr, i });
            const val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ val, ptr });
            try self.emitFmt("    call i32 (ptr, ...) @printf(ptr @.str.val, double {s})\n", .{val});
        }

        try self.emit("    call i32 (ptr, ...) @printf(ptr @.str.end)\n");
    }

    fn genSparseDecl(self: *LLVMCodegen, decl: *const ast.SparseDecl) !void {
        // Treat sparse as dense for now
        var indices = std.ArrayListUnmanaged(ast.Index){};
        defer indices.deinit(self.allocator);

        for (decl.indices) |idx| {
            const domain_name = idx.domain orelse idx.name;
            try indices.append(self.allocator, .{ .name = domain_name });
        }

        try tensor_mod.allocateTensor(self, decl.name, indices.items);
    }

    fn genBackward(self: *LLVMCodegen, backward: *const ast.Backward, program: *const ast.Program) !void {
        try self.emitFmt("\n    ; Backward pass: {s} wrt {d} params\n", .{ backward.loss, backward.params.len });

        // Build computation graph and derive gradients
        var ad = autodiff.Autodiff.init(self.allocator);
        defer ad.deinit();

        try ad.buildGraph(program);
        try ad.computeGradients(backward.loss, backward.params);

        // Generate gradient initialization (zeros)
        // First, allocate gradient tensors
        for (backward.params) |param| {
            const grad_name = try std.fmt.allocPrint(self.string_arena.allocator(), "dL_d{s}", .{param});

            // Get the original tensor info to determine shape
            if (self.tensors.get(param)) |info| {
                // Allocate gradient tensor with same shape
                try self.emitFmt("    ; Allocate gradient tensor {s}\n", .{grad_name});
                const total = info.totalSize();
                const ptr = try self.newTemp();
                try self.emitFmt("    {s} = call ptr @calloc(i64 {d}, i64 8)\n", .{ ptr, total });

                // Store tensor info
                const dims = try self.allocator.dupe(usize, info.dims);
                const strides = try self.allocator.dupe(usize, info.strides);
                try self.tensors.put(self.allocator, grad_name, .{
                    .name = grad_name,
                    .llvm_ptr = ptr,
                    .rank = info.rank,
                    .dims = dims,
                    .strides = strides,
                });
            }
        }

        // Initialize dL/dLoss = 1 (gradient of loss w.r.t. itself)
        const loss_grad = try std.fmt.allocPrint(self.string_arena.allocator(), "dL_d{s}", .{backward.loss});
        if (self.tensors.get(backward.loss)) |loss_info| {
            // Allocate loss gradient (scalar, so size 1)
            try self.emitFmt("    ; Initialize {s} = 1\n", .{loss_grad});
            const ptr = try self.newTemp();
            try self.emitFmt("    {s} = call ptr @calloc(i64 1, i64 8)\n", .{ptr});
            try self.emitFmt("    store double 1.0, ptr {s}\n", .{ptr});

            try self.tensors.put(self.allocator, loss_grad, .{
                .name = loss_grad,
                .llvm_ptr = ptr,
                .rank = loss_info.rank,
                .dims = try self.allocator.dupe(usize, loss_info.dims),
                .strides = try self.allocator.dupe(usize, loss_info.strides),
            });
        }

        // Generate gradient equations
        for (ad.grad_equations.items) |grad_eq| {
            try self.genGradEquation(&grad_eq);
        }
    }

    fn genGradEquation(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        try self.emitFmt("    ; Gradient: {s} ({s})\n", .{ grad_eq.output, @tagName(grad_eq.rule) });

        // Ensure output gradient tensor exists
        if (!self.tensors.contains(grad_eq.output)) {
            // Get the tensor we're computing gradient for (remove dL_d prefix)
            const base_name = if (std.mem.startsWith(u8, grad_eq.output, "dL_d"))
                grad_eq.output[4..]
            else
                grad_eq.output;

            if (self.tensors.get(base_name)) |info| {
                const ptr = try self.newTemp();
                try self.emitFmt("    {s} = call ptr @calloc(i64 {d}, i64 8)\n", .{ ptr, info.totalSize() });
                try self.tensors.put(self.allocator, grad_eq.output, .{
                    .name = grad_eq.output,
                    .llvm_ptr = ptr,
                    .rank = info.rank,
                    .dims = try self.allocator.dupe(usize, info.dims),
                    .strides = try self.allocator.dupe(usize, info.strides),
                });
            }
        }

        switch (grad_eq.rule) {
            .pass_through => {
                // dL/dX += dL/dY (copy upstream gradient)
                try self.genGradPassThrough(grad_eq);
            },
            .matmul_self => {
                // dL/dY from L = Y*Y -> 2*Y * dL/dL
                try self.genGradMatmulSelf(grad_eq);
            },
            .relu_grad => {
                // dL/dX += dL/dY * step(X)
                try self.genGradRelu(grad_eq);
            },
            .sigmoid_grad => {
                // dL/dX += dL/dY * Y * (1 - Y)
                try self.genGradSigmoid(grad_eq);
            },
            .matmul_left => {
                // dL/dA += dL/dC * B^T
                try self.genGradMatmulLeft(grad_eq);
            },
            .matmul_right => {
                // dL/dB += A^T * dL/dC
                try self.genGradMatmulRight(grad_eq);
            },
            else => {
                try self.emitFmt("    ; TODO: gradient rule {s}\n", .{@tagName(grad_eq.rule)});
            },
        }
    }

    fn genGradPassThrough(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // Simple: copy upstream gradient to output
        const upstream_info = self.tensors.get(grad_eq.grad_upstream) orelse return;
        const output_info = self.tensors.get(grad_eq.output) orelse return;

        const total = upstream_info.totalSize();
        for (0..total) |i| {
            const src_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ src_ptr, upstream_info.llvm_ptr, i });
            const val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ val, src_ptr });

            const dst_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ dst_ptr, output_info.llvm_ptr, i });
            const old_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, dst_ptr });
            const new_val = try self.newTemp();
            try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, val });
            try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, dst_ptr });
        }
    }

    fn genGradRelu(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // dL/dX += dL/dY * step(X)
        const upstream_info = self.tensors.get(grad_eq.grad_upstream) orelse return;
        const output_info = self.tensors.get(grad_eq.output) orelse return;
        const input_info = self.tensors.get(grad_eq.operands[0]) orelse return;

        const total = upstream_info.totalSize();
        for (0..total) |i| {
            // Load upstream gradient
            const up_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ up_ptr, upstream_info.llvm_ptr, i });
            const up_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ up_val, up_ptr });

            // Load input value
            const in_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ in_ptr, input_info.llvm_ptr, i });
            const in_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ in_val, in_ptr });

            // Compute step(X): 1 if X > 0, else 0
            const cmp = try self.newTemp();
            try self.emitFmt("    {s} = fcmp ogt double {s}, 0.0\n", .{ cmp, in_val });
            const step_val = try self.newTemp();
            try self.emitFmt("    {s} = select i1 {s}, double 1.0, double 0.0\n", .{ step_val, cmp });

            // Multiply: dL/dY * step(X)
            const grad = try self.newTemp();
            try self.emitFmt("    {s} = fmul double {s}, {s}\n", .{ grad, up_val, step_val });

            // Accumulate to output gradient
            const out_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ out_ptr, output_info.llvm_ptr, i });
            const old_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, out_ptr });
            const new_val = try self.newTemp();
            try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, grad });
            try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, out_ptr });
        }
    }

    fn genGradSigmoid(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // dL/dX += dL/dY * Y * (1 - Y)
        const upstream_info = self.tensors.get(grad_eq.grad_upstream) orelse return;
        const output_info = self.tensors.get(grad_eq.output) orelse return;
        const sigmoid_output_info = self.tensors.get(grad_eq.operands[1]) orelse return;

        const total = upstream_info.totalSize();
        for (0..total) |i| {
            // Load upstream gradient
            const up_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ up_ptr, upstream_info.llvm_ptr, i });
            const up_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ up_val, up_ptr });

            // Load sigmoid output Y
            const y_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ y_ptr, sigmoid_output_info.llvm_ptr, i });
            const y_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ y_val, y_ptr });

            // Compute Y * (1 - Y)
            const one_minus_y = try self.newTemp();
            try self.emitFmt("    {s} = fsub double 1.0, {s}\n", .{ one_minus_y, y_val });
            const deriv = try self.newTemp();
            try self.emitFmt("    {s} = fmul double {s}, {s}\n", .{ deriv, y_val, one_minus_y });

            // Multiply: dL/dY * derivative
            const grad = try self.newTemp();
            try self.emitFmt("    {s} = fmul double {s}, {s}\n", .{ grad, up_val, deriv });

            // Accumulate
            const out_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ out_ptr, output_info.llvm_ptr, i });
            const old_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, out_ptr });
            const new_val = try self.newTemp();
            try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, grad });
            try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, out_ptr });
        }
    }

    fn genGradMatmulSelf(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // For L = Y * Y (element-wise or sum of squares):
        // dL/dY[i] = 2 * Y[i] * dL/dL (but dL/dL = 1 for scalar loss)
        const output_info = self.tensors.get(grad_eq.output) orelse return;
        const input_name = grad_eq.operands[0];
        const input_info = self.tensors.get(input_name) orelse return;

        const total = input_info.totalSize();
        for (0..total) |i| {
            // Load Y[i]
            const y_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ y_ptr, input_info.llvm_ptr, i });
            const y_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ y_val, y_ptr });

            // Compute 2 * Y[i]
            const grad = try self.newTemp();
            try self.emitFmt("    {s} = fmul double {s}, 2.0\n", .{ grad, y_val });

            // Accumulate to output gradient
            const out_ptr = try self.newTemp();
            try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {d}\n", .{ out_ptr, output_info.llvm_ptr, i });
            const old_val = try self.newTemp();
            try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, out_ptr });
            const new_val = try self.newTemp();
            try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, grad });
            try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, out_ptr });
        }
    }

    fn genGradMatmulLeft(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // For C[i,k] = A[i,j] B[j,k]:
        // dL/dA[i,j] = sum_k (dL/dC[i,k] * B[j,k])
        // This is essentially: dL/dA = dL/dC @ B^T

        const output_info = self.tensors.get(grad_eq.output) orelse return;
        const upstream_info = self.tensors.get(grad_eq.grad_upstream) orelse return;

        // operands: [A, B, C] where C = A * B
        if (grad_eq.operands.len < 3) return;
        const b_name = grad_eq.operands[1];
        const b_info = self.tensors.get(b_name) orelse return;

        // Get dimensions: A[i,j], B[j,k], C[i,k]
        // dL/dA[i,j] = sum_k dL/dC[i,k] * B[j,k]
        const dim_i = if (output_info.dims.len > 0) output_info.dims[0] else 1;
        const dim_j = if (output_info.dims.len > 1) output_info.dims[1] else 1;
        const dim_k = if (upstream_info.dims.len > 1) upstream_info.dims[1] else 1;

        // Allocate loop variables
        const i_var = try self.newTemp();
        const j_var = try self.newTemp();
        const k_var = try self.newTemp();
        try self.emitFmt("    {s} = alloca i64\n", .{i_var});
        try self.emitFmt("    {s} = alloca i64\n", .{j_var});
        try self.emitFmt("    {s} = alloca i64\n", .{k_var});

        // Triple nested loop: for i, for j, for k
        const i_start = try self.newLabel();
        const i_body = try self.newLabel();
        const i_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{i_var});
        try self.emitFmt("    br label %{s}\n", .{i_start});
        try self.emitFmt("{s}:\n", .{i_start});
        const i_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ i_val, i_var });
        const i_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ i_cmp, i_val, dim_i });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ i_cmp, i_body, i_end });
        try self.emitFmt("{s}:\n", .{i_body});

        const j_start = try self.newLabel();
        const j_body = try self.newLabel();
        const j_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{j_var});
        try self.emitFmt("    br label %{s}\n", .{j_start});
        try self.emitFmt("{s}:\n", .{j_start});
        const j_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ j_val, j_var });
        const j_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ j_cmp, j_val, dim_j });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ j_cmp, j_body, j_end });
        try self.emitFmt("{s}:\n", .{j_body});

        // Inner loop over k (contracted index)
        const k_start = try self.newLabel();
        const k_body = try self.newLabel();
        const k_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{k_var});
        try self.emitFmt("    br label %{s}\n", .{k_start});
        try self.emitFmt("{s}:\n", .{k_start});
        const k_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ k_val, k_var });
        const k_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ k_cmp, k_val, dim_k });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ k_cmp, k_body, k_end });
        try self.emitFmt("{s}:\n", .{k_body});

        // Load dL/dC[i,k]
        const c_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ c_offset, i_val, dim_k });
        const c_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ c_offset2, c_offset, k_val });
        const c_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ c_ptr, upstream_info.llvm_ptr, c_offset2 });
        const dc_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ dc_val, c_ptr });

        // Load B[j,k]
        const b_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ b_offset, j_val, dim_k });
        const b_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ b_offset2, b_offset, k_val });
        const b_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ b_ptr, b_info.llvm_ptr, b_offset2 });
        const b_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ b_val, b_ptr });

        // Multiply dL/dC[i,k] * B[j,k]
        const prod = try self.newTemp();
        try self.emitFmt("    {s} = fmul double {s}, {s}\n", .{ prod, dc_val, b_val });

        // Accumulate to dL/dA[i,j]
        const a_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ a_offset, i_val, dim_j });
        const a_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ a_offset2, a_offset, j_val });
        const out_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ out_ptr, output_info.llvm_ptr, a_offset2 });
        const old_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, out_ptr });
        const new_val = try self.newTemp();
        try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, prod });
        try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, out_ptr });

        // Close k loop
        const k_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ k_next, k_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ k_next, k_var });
        try self.emitFmt("    br label %{s}\n", .{k_start});
        try self.emitFmt("{s}:\n", .{k_end});

        // Close j loop
        const j_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ j_next, j_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ j_next, j_var });
        try self.emitFmt("    br label %{s}\n", .{j_start});
        try self.emitFmt("{s}:\n", .{j_end});

        // Close i loop
        const i_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ i_next, i_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ i_next, i_var });
        try self.emitFmt("    br label %{s}\n", .{i_start});
        try self.emitFmt("{s}:\n", .{i_end});
    }

    fn genGradMatmulRight(self: *LLVMCodegen, grad_eq: *const autodiff.GradEquation) !void {
        // For C[i,k] = A[i,j] B[j,k]:
        // dL/dB[j,k] = sum_i (A[i,j] * dL/dC[i,k])
        // This is essentially: dL/dB = A^T @ dL/dC

        const output_info = self.tensors.get(grad_eq.output) orelse return;
        const upstream_info = self.tensors.get(grad_eq.grad_upstream) orelse return;

        // operands: [A, B, C] where C = A * B
        if (grad_eq.operands.len < 3) return;
        const a_name = grad_eq.operands[0];
        const a_info = self.tensors.get(a_name) orelse return;

        // Get dimensions: A[i,j], B[j,k], C[i,k]
        // dL/dB[j,k] = sum_i A[i,j] * dL/dC[i,k]
        const dim_i = if (a_info.dims.len > 0) a_info.dims[0] else 1;
        const dim_j = if (output_info.dims.len > 0) output_info.dims[0] else 1;
        const dim_k = if (output_info.dims.len > 1) output_info.dims[1] else 1;

        // Allocate loop variables
        const i_var = try self.newTemp();
        const j_var = try self.newTemp();
        const k_var = try self.newTemp();
        try self.emitFmt("    {s} = alloca i64\n", .{i_var});
        try self.emitFmt("    {s} = alloca i64\n", .{j_var});
        try self.emitFmt("    {s} = alloca i64\n", .{k_var});

        // Triple nested loop: for j, for k, for i
        const j_start = try self.newLabel();
        const j_body = try self.newLabel();
        const j_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{j_var});
        try self.emitFmt("    br label %{s}\n", .{j_start});
        try self.emitFmt("{s}:\n", .{j_start});
        const j_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ j_val, j_var });
        const j_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ j_cmp, j_val, dim_j });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ j_cmp, j_body, j_end });
        try self.emitFmt("{s}:\n", .{j_body});

        const k_start = try self.newLabel();
        const k_body = try self.newLabel();
        const k_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{k_var});
        try self.emitFmt("    br label %{s}\n", .{k_start});
        try self.emitFmt("{s}:\n", .{k_start});
        const k_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ k_val, k_var });
        const k_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ k_cmp, k_val, dim_k });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ k_cmp, k_body, k_end });
        try self.emitFmt("{s}:\n", .{k_body});

        // Inner loop over i (contracted index)
        const i_start = try self.newLabel();
        const i_body = try self.newLabel();
        const i_end = try self.newLabel();

        try self.emitFmt("    store i64 0, ptr {s}\n", .{i_var});
        try self.emitFmt("    br label %{s}\n", .{i_start});
        try self.emitFmt("{s}:\n", .{i_start});
        const i_val = try self.newTemp();
        try self.emitFmt("    {s} = load i64, ptr {s}\n", .{ i_val, i_var });
        const i_cmp = try self.newTemp();
        try self.emitFmt("    {s} = icmp slt i64 {s}, {d}\n", .{ i_cmp, i_val, dim_i });
        try self.emitFmt("    br i1 {s}, label %{s}, label %{s}\n", .{ i_cmp, i_body, i_end });
        try self.emitFmt("{s}:\n", .{i_body});

        // Load A[i,j]
        const a_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ a_offset, i_val, dim_j });
        const a_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ a_offset2, a_offset, j_val });
        const a_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ a_ptr, a_info.llvm_ptr, a_offset2 });
        const a_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ a_val, a_ptr });

        // Load dL/dC[i,k]
        const c_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ c_offset, i_val, dim_k });
        const c_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ c_offset2, c_offset, k_val });
        const c_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ c_ptr, upstream_info.llvm_ptr, c_offset2 });
        const dc_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ dc_val, c_ptr });

        // Multiply A[i,j] * dL/dC[i,k]
        const prod = try self.newTemp();
        try self.emitFmt("    {s} = fmul double {s}, {s}\n", .{ prod, a_val, dc_val });

        // Accumulate to dL/dB[j,k]
        const b_offset = try self.newTemp();
        try self.emitFmt("    {s} = mul i64 {s}, {d}\n", .{ b_offset, j_val, dim_k });
        const b_offset2 = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, {s}\n", .{ b_offset2, b_offset, k_val });
        const out_ptr = try self.newTemp();
        try self.emitFmt("    {s} = getelementptr double, ptr {s}, i64 {s}\n", .{ out_ptr, output_info.llvm_ptr, b_offset2 });
        const old_val = try self.newTemp();
        try self.emitFmt("    {s} = load double, ptr {s}\n", .{ old_val, out_ptr });
        const new_val = try self.newTemp();
        try self.emitFmt("    {s} = fadd double {s}, {s}\n", .{ new_val, old_val, prod });
        try self.emitFmt("    store double {s}, ptr {s}\n", .{ new_val, out_ptr });

        // Close i loop
        const i_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ i_next, i_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ i_next, i_var });
        try self.emitFmt("    br label %{s}\n", .{i_start});
        try self.emitFmt("{s}:\n", .{i_end});

        // Close k loop
        const k_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ k_next, k_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ k_next, k_var });
        try self.emitFmt("    br label %{s}\n", .{k_start});
        try self.emitFmt("{s}:\n", .{k_end});

        // Close j loop
        const j_next = try self.newTemp();
        try self.emitFmt("    {s} = add i64 {s}, 1\n", .{ j_next, j_val });
        try self.emitFmt("    store i64 {s}, ptr {s}\n", .{ j_next, j_var });
        try self.emitFmt("    br label %{s}\n", .{j_start});
        try self.emitFmt("{s}:\n", .{j_end});
    }
};

/// Compile a program to LLVM IR
pub fn compile(allocator: std.mem.Allocator, program: *const ast.Program) ![]const u8 {
    var codegen = LLVMCodegen.init(allocator);
    defer codegen.deinit();

    const ir = try codegen.generate(program);
    return try allocator.dupe(u8, ir);
}

// ============================================================================
// Tests
// ============================================================================

test "basic codegen init" {
    const allocator = std.testing.allocator;
    var codegen = LLVMCodegen.init(allocator);
    defer codegen.deinit();

    try codegen.emit("test\n");
    try std.testing.expectEqualStrings("test\n", codegen.output.items);
}
